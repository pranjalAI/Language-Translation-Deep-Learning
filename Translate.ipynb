{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras import layers , activations , models , preprocessing\n",
    "from tensorflow.keras import preprocessing , utils\n",
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=open(r'DATASET.txt',encoding='utf-8').read().split(\"\\n\")\n",
    "c=docs[0].strip().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_for_token = list()\n",
    "answers_for_token = list()\n",
    "c=1\n",
    "for con in docs:\n",
    "    if(c==2868):\n",
    "        pass\n",
    "    else:\n",
    "        con=con.strip().split(\"\\t\")\n",
    "        questions_for_token.append(con[0])\n",
    "        answers_for_token.append(con[1])\n",
    "        c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100 # how big is each word vector\n",
    "max_features=6000\n",
    "maxlen=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def processTweet(chat):\n",
    "    chat = chat.lower()\n",
    "    chat = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',chat)\n",
    "    chat = re.sub('@[^\\s]+','',chat)\n",
    "    chat = re.sub('[\\s]+', ' ', chat)\n",
    "    chat = re.sub(r'#([^\\s]+)', r'\\1', chat)\n",
    "    chat = re.sub(r'[\\.!:\\?\\-\\'\\\"\\\\/]', r'', chat)\n",
    "    chat = chat.strip('\\'\"')\n",
    "    return chat\n",
    "\n",
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character and replace with the character itself\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\", re.DOTALL)\n",
    "    return pattern.sub(r\"\\1\", s)\n",
    "def getFeatureVector(chat):\n",
    "    chat=processTweet(chat)\n",
    "    featureVector = []\n",
    "    #split tweet into words\n",
    "    words = chat.split()\n",
    "    for w in words:\n",
    "        #replace two or more with two occurrences\n",
    "        w = replaceTwoOrMore(w)\n",
    "        #strip punctuation\n",
    "        w = w.strip('\\'\"?,.')\n",
    "        #check if the word stats with an alphabet\n",
    "        val = re.search(r\"^[a-zA-Z][a-zA-Z0-9]*$\", w)\n",
    "        #ignore if it is a stop word\n",
    "        if(val is None):\n",
    "            continue\n",
    "        else:\n",
    "            featureVector.append(w.lower())\n",
    "    return \" \".join(list(featureVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_mat(nb_words):\n",
    "    EMBEDDING_FILE=\"glove.6B.100d.txt\"\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE, encoding=\"utf8\"))\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    emb_mean,emb_std\n",
    "\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words+1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if (i >= max_features) or i==nb_words:\n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word) #here we will get embedding for each word from GloVe\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenized_data(questions,answers,VOCAB_SIZE,tokenizer):\n",
    "    # encoder_input_data\n",
    "    import numpy as np\n",
    "    tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "    maxlen_questions = max( [ len(x) for x in tokenized_questions ] )\n",
    "    padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions , maxlen=maxlen , padding='post' )\n",
    "    encoder_input_data = np.array( padded_questions )\n",
    "    #print( encoder_input_data.shape , maxlen_questions )\n",
    "\n",
    "    # decoder_input_data\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "    maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen , padding='post' )\n",
    "    decoder_input_data = np.array( padded_answers )\n",
    "    #print( decoder_input_data.shape , maxlen_answers )\n",
    "\n",
    "    # decoder_output_data\n",
    "    tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "    for i in range(len(tokenized_answers)) :\n",
    "        tokenized_answers[i] = tokenized_answers[i][1:] # remove <start> take rest\n",
    "    padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen , padding='post' )\n",
    "    onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE)\n",
    "    decoder_output_data = np.array( onehot_answers )\n",
    "    #print( decoder_output_data.shape )\n",
    "    \n",
    "    return [encoder_input_data,decoder_input_data,decoder_output_data,maxlen_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_gpu():\n",
    "    #config = tf.compat.v1.ConfigProto( device_count = {'GPU': 1 , 'CPU': 56} ) \n",
    "    #sess = tf.compat.v1.Session(config=config) \n",
    "    #tf.compat.v1.keras.backend.set_session(\n",
    "    #    sess\n",
    "    #)\n",
    "    #gpu_options = tf.compat.v1.GPUOptions(allow_growth=True, per_process_gpu_memory_fraction=0.333)\n",
    "    #session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n",
    "    physical_devices = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(nb_words,embed_size,embedding_matrix):\n",
    "    encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "    encoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True ,weights=[embedding_matrix]) (encoder_inputs)\n",
    "    encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "    encoder_states = [ state_h , state_c ]\n",
    "\n",
    "    decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "    decoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True,weights=[embedding_matrix]) (decoder_inputs)\n",
    "    decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "    decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "\n",
    "    decoder_dense = tf.keras.layers.Dense( nb_words+1 , activation=tf.keras.activations.softmax ) \n",
    "    output = decoder_dense ( decoder_outputs )\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    decoder_outputs, state_h_d, state_c_d = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h_d, state_c_d]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "    return model,encoder_model,encoder_inputs,encoder_states,decoder_lstm,decoder_embedding,decoder_dense,decoder_inputs,decoder_outputs,decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath = \"model_Translate_new1.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(questions,answers):\n",
    "    answers=pd.DataFrame(answers, columns=[\"Ans\"])\n",
    "    questions=pd.DataFrame(questions, columns=[\"Question\"])\n",
    "    questions[\"TokQues\"]=questions[\"Question\"].apply(getFeatureVector)\n",
    "\n",
    "    answers=np.array(answers[\"Ans\"])\n",
    "    questions=np.array(questions[\"TokQues\"])\n",
    "\n",
    "    answers_with_tags = list()\n",
    "    for i in range( len( answers ) ):\n",
    "        if type( answers[i] ) == str:\n",
    "            answers_with_tags.append( answers[i] )\n",
    "        else:\n",
    "            print(questions[i])\n",
    "            print(answers[i])\n",
    "            print(type(answers[i]))\n",
    "            questions.pop(i)\n",
    "\n",
    "    answers = list()\n",
    "    for i in range( len( answers_with_tags ) ) :\n",
    "        answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )\n",
    "    \n",
    "    \n",
    "    tokenizer = preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(questions+answers)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    #embedding_matrix=emb_mat(nb_words)[0]\n",
    "    #emb_vec=emb_mat(nb_words)[1]\n",
    "\n",
    "    VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "    \n",
    "    \n",
    "    tok_out=tokenized_data(questions,answers,VOCAB_SIZE,tokenizer)\n",
    "    encoder_input_data=tok_out[0]\n",
    "    decoder_input_data=tok_out[1]\n",
    "    decoder_output_data=tok_out[2]\n",
    "    maxlen_answers=tok_out[3]\n",
    "    \n",
    "    return [encoder_input_data,decoder_input_data,decoder_output_data,maxlen_answers,nb_words,word_index,tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation done\n",
      "Encoder done\n",
      "Decoder done\n",
      "(2867, 100) (2867, 100) (2867, 100, 5375)\n",
      "Train on 2867 samples\n",
      "Epoch 1/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.4698 - accuracy: 0.2378\n",
      "Epoch 00001: loss improved from inf to 0.46957, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 62s 22ms/sample - loss: 0.4696 - accuracy: 0.2379\n",
      "Epoch 2/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.4144 - accuracy: 0.2829\n",
      "Epoch 00002: loss improved from 0.46957 to 0.41434, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 29s 10ms/sample - loss: 0.4143 - accuracy: 0.2829\n",
      "Epoch 3/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.3898 - accuracy: 0.3123\n",
      "Epoch 00003: loss improved from 0.41434 to 0.38971, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 30s 10ms/sample - loss: 0.3897 - accuracy: 0.3124\n",
      "Epoch 4/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.3698 - accuracy: 0.3400\n",
      "Epoch 00004: loss improved from 0.38971 to 0.36997, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 30s 11ms/sample - loss: 0.3700 - accuracy: 0.3399\n",
      "Epoch 5/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.3514 - accuracy: 0.3652\n",
      "Epoch 00005: loss improved from 0.36997 to 0.35150, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 30s 10ms/sample - loss: 0.3515 - accuracy: 0.3653\n",
      "Epoch 6/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.3338 - accuracy: 0.3895\n",
      "Epoch 00006: loss improved from 0.35150 to 0.33344, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 29s 10ms/sample - loss: 0.3334 - accuracy: 0.3898\n",
      "Epoch 7/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.3172 - accuracy: 0.4145\n",
      "Epoch 00007: loss improved from 0.33344 to 0.31710, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 26s 9ms/sample - loss: 0.3171 - accuracy: 0.4147\n",
      "Epoch 8/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.3009 - accuracy: 0.4376\n",
      "Epoch 00008: loss improved from 0.31710 to 0.30075, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.3008 - accuracy: 0.4377\n",
      "Epoch 9/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2864 - accuracy: 0.4633\n",
      "Epoch 00009: loss improved from 0.30075 to 0.28635, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2864 - accuracy: 0.4634\n",
      "Epoch 10/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2727 - accuracy: 0.4882\n",
      "Epoch 00010: loss improved from 0.28635 to 0.27275, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 22s 8ms/sample - loss: 0.2728 - accuracy: 0.4881\n",
      "Epoch 11/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.5122\n",
      "Epoch 00011: loss improved from 0.27275 to 0.25932, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2593 - accuracy: 0.5118\n",
      "Epoch 12/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2464 - accuracy: 0.5423\n",
      "Epoch 00012: loss improved from 0.25932 to 0.24653, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2465 - accuracy: 0.5421\n",
      "Epoch 13/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2351 - accuracy: 0.5650\n",
      "Epoch 00013: loss improved from 0.24653 to 0.23499, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2350 - accuracy: 0.5650\n",
      "Epoch 14/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.5902\n",
      "Epoch 00014: loss improved from 0.23499 to 0.22420, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2242 - accuracy: 0.5901\n",
      "Epoch 15/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2143 - accuracy: 0.6107\n",
      "Epoch 00015: loss improved from 0.22420 to 0.21434, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2143 - accuracy: 0.6105\n",
      "Epoch 16/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.2054 - accuracy: 0.6311\n",
      "Epoch 00016: loss improved from 0.21434 to 0.20528, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.2053 - accuracy: 0.6313\n",
      "Epoch 17/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.1965 - accuracy: 0.6501\n",
      "Epoch 00017: loss improved from 0.20528 to 0.19645, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 24s 8ms/sample - loss: 0.1965 - accuracy: 0.6502\n",
      "Epoch 18/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.1886 - accuracy: 0.6722\n",
      "Epoch 00018: loss improved from 0.19645 to 0.18856, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 24s 8ms/sample - loss: 0.1886 - accuracy: 0.6722\n",
      "Epoch 19/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.1815 - accuracy: 0.6896\n",
      "Epoch 00019: loss improved from 0.18856 to 0.18134, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 24s 8ms/sample - loss: 0.1813 - accuracy: 0.6897\n",
      "Epoch 20/20\n",
      "2860/2867 [============================>.] - ETA: 0s - loss: 0.1751 - accuracy: 0.7045\n",
      "Epoch 00020: loss improved from 0.18134 to 0.17512, saving model to model_Translate_new1.h5\n",
      "2867/2867 [==============================] - 23s 8ms/sample - loss: 0.1751 - accuracy: 0.7047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x200469346c8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prepared_data=prepare_data(questions_for_token,answers_for_token)\n",
    "print(\"Preparation done\")\n",
    "encoder_input_data=Prepared_data[0]\n",
    "print(\"Encoder done\")\n",
    "decoder_input_data=Prepared_data[1]\n",
    "decoder_output_data=Prepared_data[2]\n",
    "print(\"Decoder done\")\n",
    "maxlen_answers=Prepared_data[3]\n",
    "nb_words=Prepared_data[4]\n",
    "word_index=Prepared_data[5]\n",
    "tokenizer=Prepared_data[6]\n",
    "print(encoder_input_data.shape, decoder_input_data.shape, decoder_output_data.shape)\n",
    "\n",
    "embedding_matrix=emb_mat(nb_words)\n",
    "#model=get_model(nb_words,embed_size,embedding_matrix)[0]\n",
    "#encoder_model=get_model(nb_words,embed_size,embedding_matrix)[1]\n",
    "#encoder_inputs=get_model(nb_words,embed_size,embedding_matrix)[2]\n",
    "#encoder_states=get_model(nb_words,embed_size,embedding_matrix)[3]\n",
    "#decoder_lstm=get_model(nb_words,embed_size,embedding_matrix)[4]\n",
    "#decoder_embedding=get_model(nb_words,embed_size,embedding_matrix)[5]\n",
    "#decoder_dense=get_model(nb_words,embed_size,embedding_matrix)[6]\n",
    "#decoder_inputs=get_model(nb_words,embed_size,embedding_matrix)[7]\n",
    "#decoder_outputs=get_model(nb_words,embed_size,embedding_matrix)[8]\n",
    "#decoder_model=get_model(nb_words,embed_size,embedding_matrix)[9]\n",
    "\n",
    "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True, weights=[embedding_matrix]) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 200 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( nb_words+1, embed_size , mask_zero=True,weights=[embedding_matrix]) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 200 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "\n",
    "decoder_dense = tf.keras.layers.Dense( nb_words+1 , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit([encoder_input_data , decoder_input_data], decoder_output_data, batch_size=10, epochs=20, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.models import load_model\n",
    "#Loaded_model = load_model(r'model_Translate_new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 200 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    decoder_states = [state_h, state_c]\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen , padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_model , dec_model = make_inference_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter question : what are you doing\n",
      "क्या तुम कैसे रहे हो\n",
      "Enter question : how are you?\n",
      "Sorry, I don't understand. Please try again.\n",
      "Enter question : ow are you\n",
      "Sorry, I don't understand. Please try again.\n",
      "Enter question : how are you\n",
      "आप कैसे हैं\n",
      "Enter question : i am good\n",
      "मैं अच्छा हूँ।\n",
      "Enter question : what can you do for me\n",
      "क्या तुम मुझे यकीन करते हो\n",
      "Enter question : i do not\n",
      "मैं नहीं कर सकता हूँ।\n",
      "Enter question : ok do not do it\n",
      "क्या कर रहा है क्या\n",
      "Enter question : where do you live\n",
      "तुम कहाँ जा रहे हो\n",
      "Enter question : where are you going\n",
      "तुम कहाँ जा रहे हो\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    try:\n",
    "        states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "        empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "        stop_condition = False\n",
    "        decoded_translation = ''\n",
    "        while not stop_condition :\n",
    "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "            sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "            sampled_word = None\n",
    "            for word , index in tokenizer.word_index.items() :\n",
    "                if sampled_word_index == index :\n",
    "                    decoded_translation += ' {}'.format( word )\n",
    "                    sampled_word = word\n",
    "            \n",
    "            if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "                stop_condition = True\n",
    "                \n",
    "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "            states_values = [ h , c ] \n",
    "\n",
    "        print( \" \".join(decoded_translation.strip().split(\" \")[:-1]) )\n",
    "    except:\n",
    "        print(\"Sorry, I don't understand. Please try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still a better accuracy with 20 epoches. We can make it better with more data and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 GPU",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
